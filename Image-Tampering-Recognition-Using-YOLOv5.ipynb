{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Tampering Recognition Algorithm Based on Improved YOLOv5s\n\n### A paper by: Zhen Liu ","metadata":{}},{"cell_type":"markdown","source":"### **Author**: Tadesse Abateneh","metadata":{}},{"cell_type":"markdown","source":"# Object Detection Task in Computer Vision\n\nObject detection, a fundamental task in computer vision, involves identifying objects within images or video frames. While humans effortlessly recognize objects in their surroundings, computers require sophisticated algorithms to achieve similar capabilities. Object detection for computers entails two primary objectives: classification, which involves identifying the type of objects present, and localization, which determines the precise location of these objects within the image.\n\nVarious approaches have been devised to address this challenge, with one leading algorithm being YOLO (You Only Look Once). Renowned for its real-time accuracy, YOLO stands out among contemporary solutions. In this exploration, we will delve into training YOLO on custom datasets using Pytorch.","metadata":{}},{"cell_type":"markdown","source":"## YOLO: A Swift Real-Time Object Detector\n\n### Understanding YOLO\n\n**`YOLO`**, short for **\"You Only Look Once,\"** revolutionizes object detection by swiftly identifying all objects within an image through a single algorithm run. This is achieved by dividing the image into a grid, where each cell predicts bounding boxes and class probabilities.\n\nThe raw output of YOLO often yields multiple bounding boxes for the same object, varying in shape and size. To refine these predictions, a Non-maximum suppression (`NMS`) algorithm is employed. NMS utilizes confidence levels associated with each predicted box to eliminate those with low certainty, typically below `0.5`. Among the remaining high-confidence boxes, NMS selects the optimal one based on intersection calculations with neighboring boxes, ensuring accurate object localization.\n\n#### `YOLO` versus Other Detectors\n\nWhile `YOLO` employs a convolutional neural network (`CNN`) like many other detectors, its distinctive single-stage approach enables real-time performance. In contrast, slower algorithms such as Faster R-CNN adopt a two-stage methodology. Firstly, they identify interesting image regions potentially containing objects, followed by classification using a CNN. This two-stage process, however, is more time-consuming as it involves classifying numerous regions individually. YOLO circumvents this by directly predicting bounding boxes and classes for the entire image in a single forward pass, eliminating the need for region selection and significantly enhancing speed.","metadata":{}},{"cell_type":"markdown","source":"# Introduction: Implementing `YOLOv5` for Effective Object Detection\n\nWelcome to our `YOLOv5` implementation notebook! \n\n`YOLOv5` is a cutting-edge object detection model celebrated for its speed and accuracy. In this notebook, we'll explore its architecture, implementation using PyTorch, and training on custom datasets. We'll also optimize inference speed for seamless integration into real-world applications. \n\nLet's dive in!","metadata":{}},{"cell_type":"markdown","source":"### Clone YOLOv5 model","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5\n%cd yolov5","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:57:30.984643Z","iopub.execute_input":"2024-03-19T05:57:30.984996Z","iopub.status.idle":"2024-03-19T05:57:33.865482Z","shell.execute_reply.started":"2024-03-19T05:57:30.984966Z","shell.execute_reply":"2024-03-19T05:57:33.864448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Install Necessary Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -qr requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T05:57:33.867513Z","iopub.execute_input":"2024-03-19T05:57:33.867804Z","iopub.status.idle":"2024-03-19T05:57:48.167418Z","shell.execute_reply.started":"2024-03-19T05:57:33.867778Z","shell.execute_reply":"2024-03-19T05:57:48.166113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install roboflow","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:57:48.169037Z","iopub.execute_input":"2024-03-19T05:57:48.169421Z","iopub.status.idle":"2024-03-19T05:58:04.462742Z","shell.execute_reply.started":"2024-03-19T05:57:48.169392Z","shell.execute_reply":"2024-03-19T05:58:04.461783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install clearml>=1.2.0","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:58:04.464310Z","iopub.execute_input":"2024-03-19T05:58:04.465151Z","iopub.status.idle":"2024-03-19T05:58:18.606205Z","shell.execute_reply.started":"2024-03-19T05:58:04.465111Z","shell.execute_reply":"2024-03-19T05:58:18.604951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Integrating ClearML for visualization","metadata":{}},{"cell_type":"code","source":"%env CLEARML_WEB_HOST=https://app.clear.ml\n%env CLEARML_API_HOST=https://api.clear.ml\n%env CLEARML_FILES_HOST=https://files.clear.ml\n%env CLEARML_API_ACCESS_KEY=\"Insert your API Key\"\n%env CLEARML_API_SECRET_KEY=\"Insert your Sectret Key\"","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:58:18.609141Z","iopub.execute_input":"2024-03-19T05:58:18.609455Z","iopub.status.idle":"2024-03-19T05:58:18.619635Z","shell.execute_reply.started":"2024-03-19T05:58:18.609429Z","shell.execute_reply":"2024-03-19T05:58:18.618726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom roboflow import Roboflow\nimport yaml\nfrom IPython.core.magic import register_line_cell_magic\nfrom utils.plots import plot_results  \nfrom IPython.display import Image, clear_output  # to display images\nfrom utils.downloads import attempt_download  # to download models/datasets\n\n\n# clear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:58:18.620643Z","iopub.execute_input":"2024-03-19T05:58:18.620893Z","iopub.status.idle":"2024-03-19T05:58:26.620431Z","shell.execute_reply.started":"2024-03-19T05:58:18.620870Z","shell.execute_reply":"2024-03-19T05:58:26.619344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CBAM Attention Module","metadata":{}},{"cell_type":"code","source":"# Define CBAM Module\nclass ChannelAttention(nn.Module):\n    \"\"\"\n    Channel Attention Module\n    \"\"\"\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    \"\"\"\n    Spatial Attention Module\n    \"\"\"\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv(x)\n        return self.sigmoid(x)\n\nclass CBAMBlock(nn.Module):\n    \"\"\"\n    CBAM (Convolutional Block Attention Module) Block\n    \"\"\"\n    def __init__(self, in_planes, ratio=16):\n        super(CBAMBlock, self).__init__()\n        self.ca = ChannelAttention(in_planes, ratio)\n        self.sa = SpatialAttention()\n\n    def forward(self, x):\n        x = x * self.ca(x)\n        x = x * self.sa(x)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:58:26.621736Z","iopub.execute_input":"2024-03-19T05:58:26.622280Z","iopub.status.idle":"2024-03-19T05:58:26.635764Z","shell.execute_reply.started":"2024-03-19T05:58:26.622245Z","shell.execute_reply":"2024-03-19T05:58:26.634735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimization of Loss Funciton - CIOU Loss Funtion","metadata":{}},{"cell_type":"code","source":"# Define CIoU Loss Function\ndef compute_ciou_loss(pred_boxes, target_boxes):\n    \"\"\"\n    Compute CIoU (Complete Intersection over Union) Loss\n    \"\"\"\n    # Extract coordinates for ease of calculation\n    x1, y1, w1, h1 = pred_boxes[..., 0], pred_boxes[..., 1], pred_boxes[..., 2], pred_boxes[..., 3]\n    x2, y2, w2, h2 = target_boxes[..., 0], target_boxes[..., 1], target_boxes[..., 2], target_boxes[..., 3]\n\n    # Compute intersection and union areas\n    x_left = torch.max(x1 - w1 / 2, x2 - w2 / 2)\n    y_top = torch.max(y1 - h1 / 2, y2 - h2 / 2)\n    x_right = torch.min(x1 + w1 / 2, x2 + w2 / 2)\n    y_bottom = torch.min(y1 + h1 / 2, y2 + h2 / 2)\n\n    intersection = torch.clamp((x_right - x_left), min=0) * torch.clamp((y_bottom - y_top), min=0)\n    union = w1 * h1 + w2 * h2 - intersection\n\n    # Compute IoU\n    iou = intersection / union\n\n    # Compute center distance\n    center_distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2)\n\n    # Compute enclosed area\n    enclosed_area = torch.min(w1, w2) * torch.min(h1, h2)\n\n    # Compute CIoU\n    ciou = iou - (center_distance / (enclosed_area + 1e-7))\n\n    # Compute loss\n    ciou_loss = 1 - ciou\n\n    return ciou_loss.mean()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:58:26.637258Z","iopub.execute_input":"2024-03-19T05:58:26.637642Z","iopub.status.idle":"2024-03-19T05:58:26.657688Z","shell.execute_reply.started":"2024-03-19T05:58:26.637610Z","shell.execute_reply":"2024-03-19T05:58:26.656735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Loading from Roboflow","metadata":{}},{"cell_type":"markdown","source":"#### Citation of Dataset\n\n```python\n\n@misc{\n    forge-eq4rh_dataset,\n    title = { forge Dataset },\n    type = { Open Source Dataset },\n    author = { Pavan Kumar },\n    howpublished = { \\url{ https://universe.roboflow.com/pavan-kumar/forge-eq4rh } },\n    url = { https://universe.roboflow.com/pavan-kumar/forge-eq4rh },\n    journal = { Roboflow Universe },\n    publisher = { Roboflow },\n    year = { 2023 },\n    month = { nov },\n    note = { visited on 2024-02-29 },\n    }\n\n```\n\n### Dataset Count\n\n- **Total:** `7257`\n- **Train set:** `5075 (70%)`\n- **Valid set:** `1459 (20%)`\n- **Test set:** `723 (10%)`","metadata":{}},{"cell_type":"code","source":"# Roboflow Dataset Download\nrf = Roboflow(api_key=\"FKec27mGI5ejI1KBxPZI\")\nproject = rf.workspace(\"pavan-kumar\").project(\"forge-eq4rh\")\ndataset = project.version(1).download(\"yolov5\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:58:26.658775Z","iopub.execute_input":"2024-03-19T05:58:26.659036Z","iopub.status.idle":"2024-03-19T05:59:09.742585Z","shell.execute_reply.started":"2024-03-19T05:58:26.659013Z","shell.execute_reply":"2024-03-19T05:59:09.741611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### YAML Configuration","metadata":{}},{"cell_type":"code","source":"# Load YAML file\nwith open(dataset.location + \"/data.yaml\", 'r') as stream:\n    num_classes = str(yaml.safe_load(stream)['nc'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:59:09.744802Z","iopub.execute_input":"2024-03-19T05:59:09.745149Z","iopub.status.idle":"2024-03-19T05:59:09.752877Z","shell.execute_reply.started":"2024-03-19T05:59:09.745108Z","shell.execute_reply":"2024-03-19T05:59:09.751884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Configuration\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:59:09.754394Z","iopub.execute_input":"2024-03-19T05:59:09.754803Z","iopub.status.idle":"2024-03-19T05:59:09.765421Z","shell.execute_reply.started":"2024-03-19T05:59:09.754769Z","shell.execute_reply":"2024-03-19T05:59:09.764617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Customize the Configration of the Model","metadata":{}},{"cell_type":"code","source":"%%writetemplate /kaggle/working/yolov5/models/custom_yolov5s.yaml\n\n# parameters\nnc: {num_classes}  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\n\n# anchors\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n\n# YOLOv5 backbone with CBAM\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, BottleneckCSP, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 9, BottleneckCSP, [256]],\n   #[-1, 1, CBAMBlock, [512]],  # CBAM added here\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, BottleneckCSP, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n  ]\n\n# YOLOv5 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, BottleneckCSP, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:59:09.766935Z","iopub.execute_input":"2024-03-19T05:59:09.767267Z","iopub.status.idle":"2024-03-19T05:59:09.776903Z","shell.execute_reply.started":"2024-03-19T05:59:09.767236Z","shell.execute_reply":"2024-03-19T05:59:09.776063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"# Train YOLOv5s on custom data for 200 epochs\n# Time its performance\n%time\n%cd /kaggle/working/yolov5\n!python train.py --img 416 --batch 32 --epochs 50 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights 'yolov5s.pt' --name yolov5s_results  --cache\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:59:09.777946Z","iopub.execute_input":"2024-03-19T05:59:09.778226Z","iopub.status.idle":"2024-03-19T06:43:48.107275Z","shell.execute_reply.started":"2024-03-19T05:59:09.778192Z","shell.execute_reply":"2024-03-19T06:43:48.106107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `Epoch`: This indicates the current epoch of training.\n- `GPU_mem`: Represents the GPU memory usage during training.\n- `box_loss`, `obj_loss`, `cls_loss`: These represent the losses associated with bounding box coordinates, objectness prediction, and class prediction, respectively.\n- `Instances`: Refers to the number of instances detected.\n- `Size`: Indicates the input size of the images being processed during training.\n- `Class`, `Images`, `Instances`, `P`, `R`, `mAP50`: These metrics provide evaluation results on the validation set.\n    - `Class`: Indicates the class being evaluated.\n    - `Images`: The number of images evaluated.\n    - `Instances`: Total instances of the class.\n    - `P`: Precision for the class.\n    - `R`: Recall for the class.\n    - `mAP50`: Mean Average Precision for the class at IoU (Intersection over Union) threshold of 0.5.","metadata":{}},{"cell_type":"markdown","source":"### Detection","metadata":{}},{"cell_type":"code","source":"# Detection using YOLOv5s on custom test set\n%cd /kaggle/working/yolov5/\n!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source /kaggle/working/yolov5/forge-1/test/images","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:43:48.112494Z","iopub.execute_input":"2024-03-19T06:43:48.112817Z","iopub.status.idle":"2024-03-19T06:44:58.266290Z","shell.execute_reply.started":"2024-03-19T06:43:48.112788Z","shell.execute_reply":"2024-03-19T06:44:58.265136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"# Validate YOLOv5s on custom data validation set\n%cd /kaggle/working/yolov5\n%time  # Measure time taken\n!python val.py --weights runs/train/yolov5s_results/weights/best.pt \\\n               --data {dataset.location}/data.yaml \\\n               --img 640 \\\n               --half\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:44:58.267710Z","iopub.execute_input":"2024-03-19T06:44:58.268015Z","iopub.status.idle":"2024-03-19T06:45:57.148300Z","shell.execute_reply.started":"2024-03-19T06:44:58.267988Z","shell.execute_reply":"2024-03-19T06:45:57.147135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization of Training Data","metadata":{}},{"cell_type":"code","source":"# Plot results.txt as results.png\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # View results.png","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.152990Z","iopub.execute_input":"2024-03-19T06:45:57.154050Z","iopub.status.idle":"2024-03-19T06:45:57.169593Z","shell.execute_reply.started":"2024-03-19T06:45:57.154017Z","shell.execute_reply":"2024-03-19T06:45:57.168713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out an augmented training example\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/train_batch0.jpg', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.170832Z","iopub.execute_input":"2024-03-19T06:45:57.171136Z","iopub.status.idle":"2024-03-19T06:45:57.186555Z","shell.execute_reply.started":"2024-03-19T06:45:57.171110Z","shell.execute_reply":"2024-03-19T06:45:57.185558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out an augmented training example\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/train_batch2.jpg', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.187774Z","iopub.execute_input":"2024-03-19T06:45:57.188041Z","iopub.status.idle":"2024-03-19T06:45:57.206961Z","shell.execute_reply.started":"2024-03-19T06:45:57.188016Z","shell.execute_reply":"2024-03-19T06:45:57.206110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg', width=900)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.208235Z","iopub.execute_input":"2024-03-19T06:45:57.208873Z","iopub.status.idle":"2024-03-19T06:45:57.221226Z","shell.execute_reply.started":"2024-03-19T06:45:57.208840Z","shell.execute_reply":"2024-03-19T06:45:57.220201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics Plots","metadata":{}},{"cell_type":"markdown","source":"#### Precision (P):\n\nPrecision measures the accuracy of the positive predictions made by the model. It is calculated as the ratio of true positive predictions to the total number of positive predictions made by the model. In object detection, precision indicates how many of the detected objects are relevant (true positives) out of all the objects detected by the model.","metadata":{}},{"cell_type":"code","source":"# Print out P Curve - Precision\n\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/P_curve.png', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.222487Z","iopub.execute_input":"2024-03-19T06:45:57.222793Z","iopub.status.idle":"2024-03-19T06:45:57.232002Z","shell.execute_reply.started":"2024-03-19T06:45:57.222765Z","shell.execute_reply":"2024-03-19T06:45:57.231115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Recall (R):\n\nRecall measures the ability of the model to correctly detect all relevant instances of objects. It is calculated as the ratio of true positive predictions to the total number of actual positive instances present in the dataset. In object detection, recall indicates how many of the actual objects were detected by the model.","metadata":{}},{"cell_type":"code","source":"# Print out R Curve - Recall\n\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/R_curve.png', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.233133Z","iopub.execute_input":"2024-03-19T06:45:57.233467Z","iopub.status.idle":"2024-03-19T06:45:57.242915Z","shell.execute_reply.started":"2024-03-19T06:45:57.233432Z","shell.execute_reply":"2024-03-19T06:45:57.241875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Precision-Recall (PR) Curve:\n\nA Precision-Recall curve is a graphical representation of the trade-off between precision and recall for different threshold values used in the object detection model. It is created by plotting precision on the y-axis against recall on the x-axis for various threshold values. The curve helps in understanding how the model's performance varies as the threshold for detection changes.","metadata":{}},{"cell_type":"code","source":"# Print out PR Curve - Precision Recall\n\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/PR_curve.png', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.243915Z","iopub.execute_input":"2024-03-19T06:45:57.244183Z","iopub.status.idle":"2024-03-19T06:45:57.253809Z","shell.execute_reply.started":"2024-03-19T06:45:57.244159Z","shell.execute_reply":"2024-03-19T06:45:57.252817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### F1 Score:\n\nThe F1 Score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall.\n\nF1 Score ranges from 0 to 1, where a higher value indicates better performance, with 1 being the best possible score.","metadata":{}},{"cell_type":"code","source":"# Print out F1 score\n\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/F1_curve.png', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.255043Z","iopub.execute_input":"2024-03-19T06:45:57.255460Z","iopub.status.idle":"2024-03-19T06:45:57.264582Z","shell.execute_reply.started":"2024-03-19T06:45:57.255426Z","shell.execute_reply":"2024-03-19T06:45:57.263659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix\n\nA confusion matrix is a table that is often used to evaluate the performance of a classification model. It summarizes the predictions made by the model on a test dataset in comparison to the actual ground truth labels. However, in the context of object detection tasks such as those performed by YOLOv5, the confusion matrix concept is not as straightforward as in traditional classification tasks where each class is mutually exclusive.\n\nInstead, in object detection tasks, a confusion matrix is typically adapted to account for the bounding boxes and the objects they represent. It may include metrics like:\n\n- **True Positives (TP)**: The model correctly predicts the presence of an object in an image.\n- **False Positives (FP)**: The model predicts the presence of an object when there is none.\n- **False Negatives (FN)**: The model fails to predict the presence of an object when it is actually present.\n- **True Negatives (TN)**: Not usually applicable in object detection tasks.\n\nHowever, since object detection tasks involve not just binary classification but also localization, the calculation of these metrics can be more complex. The bounding boxes need to be evaluated in terms of their location, size, and overlap with ground truth boxes.\n\nA confusion matrix curve, if it exists, would likely plot the changes in these metrics (TP, FP, FN) as the threshold for detection varies. This could help in understanding how the model's performance changes with different confidence thresholds for object detection.","metadata":{}},{"cell_type":"code","source":"# Print out confusion matrix\nImage(filename='/kaggle/working/yolov5/runs/train/yolov5s_results/confusion_matrix.png', width=900)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:45:57.265805Z","iopub.execute_input":"2024-03-19T06:45:57.266167Z","iopub.status.idle":"2024-03-19T06:45:57.275295Z","shell.execute_reply.started":"2024-03-19T06:45:57.266101Z","shell.execute_reply":"2024-03-19T06:45:57.274273Z"},"trusted":true},"execution_count":null,"outputs":[]}]}